import streamlit as st
import requests
import json
from bs4 import BeautifulSoup
import trafilatura
from collections import Counter
import re
import pandas as pd

# –¢–≤–æ–π –∫–ª—é—á API Serper
SERPER_API_KEY = "c0d0cfb9aa136fa3f4818f973da0617602910ef6"

def get_data(query):
    url = "https://google.serper.dev/search"
    payload = json.dumps({"q": query, "gl": "ru", "hl": "ru"})
    headers = {'X-API-KEY': SERPER_API_KEY, 'Content-Type': 'application/json'}
    try:
        response = requests.request("POST", url, headers=headers, data=payload)
        return response.json().get('organic', [])[:7]
    except:
        return []

def analyze_page(url):
    try:
        downloaded = trafilatura.fetch_url(url)
        if not downloaded: return None
        text = trafilatura.extract(downloaded)
        soup = BeautifulSoup(downloaded, 'html.parser')
        headers = [h.get_text().strip() for h in soup.find_all(['h2', 'h3']) if len(h.get_text()) > 10]
        return {"title": soup.title.string if soup.title else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞", "text": text, "headers": headers, "words": len(text.split()) if text else 0}
    except: return None

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞ —Å–∞–π—Ç–∞
st.set_page_config(page_title="SEO TZ Generator", layout="wide")
st.title("üöÄ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¢–ó –¥–ª—è –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä–∞")
st.write("–ê–Ω–∞–ª–∏–∑ –¢–û–ü-7 –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π.")

query = st.text_input("–í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (—Ç–µ–º—É —Å—Ç–∞—Ç—å–∏):")

if st.button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¢–ó"):
    if not query:
        st.warning("–°–Ω–∞—á–∞–ª–∞ –≤–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å!")
    else:
        with st.spinner('–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –≤—ã–¥–∞—á—É Google...'):
            results = get_data(query)
            data_list, all_texts, all_headers = [], [], []

            for res in results:
                page = analyze_page(res['link'])
                if page:
                    page['url'] = res['link']
                    data_list.append(page)
                    all_texts.append(page['text'])
                    all_headers.extend(page['headers'])

            if data_list:
                df = pd.DataFrame(data_list)[['title', 'words', 'url']]
                st.subheader("üìä –ê–Ω–∞–ª–∏–∑ –¢–û–ü-7 –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
                st.dataframe(df, use_container_width=True)
                
                avg_words = int(df['words'].mean())
                st.success(f"üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –æ–±—ä–µ–º: ~{avg_words} —Å–ª–æ–≤ (–ø—Ä–∏–º–µ—Ä–Ω–æ {avg_words*7} –∑–Ω–∞–∫–æ–≤)")

                col1, col2 = st.columns(2)
                with col1:
                    st.subheader("üîë –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (LSI)")
                    full_text = " ".join(all_texts).lower()
                    words = re.findall(r'\b[–∞-—è—ë]{5,15}\b', full_text)
                    st.write(", ".join([k[0] for k in Counter(words).most_common(25)]))

                with col2:
                    st.subheader("üìë –ó–∞–≥–æ–ª–æ–≤–∫–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
                    for h in list(set(all_headers))[:15]:
                        st.write(f"- {h}")
            else:
                st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –∑–∞–ø—Ä–æ—Å.")
